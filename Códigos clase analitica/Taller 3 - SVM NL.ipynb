{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargue de librerias\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd  \n",
    "from sklearn import svm     #Import svm model\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lectura de datos de entrenamiento\n",
    "# Estos datos fueron generados aleatoriamente para representar datos que fueran linealmente separables\n",
    "\n",
    "dat = pd.read_csv(r'C:\\Users\\jgarcia.d\\datos\\dataNL.csv', delimiter=';', decimal=',')  # Cargar el dataset\n",
    "dat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(dat.x1, dat.x2, c=dat.y, cmap=plt.cm.coolwarm);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separamos las variables independientes de la dependiente, en diferentes datasets\n",
    "\n",
    "X = dat.drop('y', axis=1)\n",
    "y = dat['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparación de datos de entrenamiento y datos de prueba\n",
    "\n",
    "# Importamos la función train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Dividimos los datasets en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.50,random_state=109) # 50% training and 50% test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_train.x1, X_train.x2, c=y_train, cmap=plt.cm.coolwarm);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_test.x1, X_test.x2, c=y_test, cmap=plt.cm.coolwarm);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generamos un primer modelo LINEAL\n",
    "clf = svm.SVC(kernel='linear', C=10, random_state=109) # Linear Kernel\n",
    "\n",
    "#Entrenamos el modelo usando el dataset de entrenamiento\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "#Pronosticamos la respuesta para el dataset de prueba\n",
    "y_predl = clf.predict(X_test)\n",
    "\n",
    "#Importamos la librería para cálculo de métricas de desempeño \n",
    "from sklearn import metrics\n",
    "\n",
    "# Cálculo de accuracy del modelo:  Qué tan frecuentemente el clasificador es correcto?  Con el dataset de prueba.\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_predl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importamos la librería para cálculo de la matriz de confusión \n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cml = confusion_matrix(y_test, y_predl)\n",
    "print('Confusion Matrix : \\n', cml)\n",
    "\n",
    "total1=sum(sum(cml))\n",
    "##### A partir de la matriz de confusión calculamos accuracy, sensitivity, specificity\n",
    "accuracyl=(cml[0,0]+cml[1,1])/total1\n",
    "print ('Accuracy : ', accuracyl)\n",
    "\n",
    "sensitivityl = cml[0,0]/(cml[0,0]+cml[0,1])\n",
    "print('Sensitivity : ', sensitivityl )\n",
    "\n",
    "specificityl = cml[1,1]/(cml[1,0]+cml[1,1])\n",
    "print('Specificity : ', specificityl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print (classification_report(y_test, y_predl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datos del hiperplano de separación\n",
    "\n",
    "w = clf.coef_[0]\n",
    "a = -w[0] / w[1]\n",
    "xx = np.linspace(-5, 5)\n",
    "yy = a * xx - (clf.intercept_[0]) / w[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.intercept_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráfico del hiperplano y el margen de sepración\n",
    "# \n",
    "b = clf.support_vectors_[0]\n",
    "yy_down = a * xx + (b[1] - a * b[0])\n",
    "b = clf.support_vectors_[-1]\n",
    "yy_up = a * xx + (b[1] - a * b[0])\n",
    "\n",
    "plt.plot(xx, yy, 'k-')\n",
    "plt.plot(xx, yy_down, 'k--')\n",
    "plt.plot(xx, yy_up, 'k--')\n",
    "\n",
    "plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1],\n",
    "            s=80, facecolors='none')\n",
    "plt.scatter(X.x1, X.x2, c=y, cmap=plt.cm.Paired)\n",
    "\n",
    "plt.axis('tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graficamos el SVM con el hiperplano de separación, y el margen\n",
    "\n",
    "def plot_svc_decision_function(model, ax=None, plot_support=True):\n",
    "    \"\"\"Plot the decision function for a 2D SVC\"\"\"\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = ax.get_ylim()\n",
    "    \n",
    "    # create grid to evaluate model\n",
    "    x = np.linspace(xlim[0], xlim[1], 30)\n",
    "    y = np.linspace(ylim[0], ylim[1], 30)\n",
    "    Y, X = np.meshgrid(y, x)\n",
    "    xy = np.vstack([X.ravel(), Y.ravel()]).T\n",
    "    P = model.decision_function(xy).reshape(X.shape)\n",
    "    \n",
    "    # plot decision boundary and margins\n",
    "    ax.contour(X, Y, P, colors='k',\n",
    "               levels=[-1, 0, 1], alpha=0.5,\n",
    "               linestyles=['--', '-', '--'])\n",
    "    \n",
    "    # plot support vectors\n",
    "    if plot_support:\n",
    "        ax.scatter(model.support_vectors_[:, 0],\n",
    "                   model.support_vectors_[:, 1],\n",
    "                   s=300, linewidth=1, facecolors='none');\n",
    "    ax.set_xlim(xlim)\n",
    "    ax.set_ylim(ylim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X.x1, X.x2, c=y, cmap=plt.cm.coolwarm)\n",
    "plot_svc_decision_function(clf);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generamos un primer clasificador svm RADIAL\n",
    "clfr = svm.SVC(kernel='rbf')\n",
    "#Entrenamos el modelo usando los datos de entrenamiento\n",
    "clfr.fit(X_train, y_train)\n",
    "print(\"gamma: \",clfr._gamma)\n",
    "\n",
    "#Pronosticamos la respuesta para el dataset de prueba\n",
    "#En entrenamiento\n",
    "y_predr_e = clfr.predict(X_train)\n",
    "#En prueba\n",
    "y_predr = clfr.predict(X_test)\n",
    "\n",
    "#Accuracy del modelo:  Qué tan frecuentemente el clasificador es correcto?\n",
    "#En entrenamiento: \n",
    "print(\"Accuracy - entrenamiento: \",metrics.accuracy_score(y_train, y_predr_e))\n",
    "#En prueba: \n",
    "print(\"Accuracy - prueba: \",metrics.accuracy_score(y_test, y_predr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Analisis de desempeño del modelo con datos de prueba\n",
    "\n",
    "cmr = confusion_matrix(y_test, y_predr)\n",
    "print('Confusion Matrix : \\n', cmr)\n",
    "total1=sum(sum(cmr))\n",
    "\n",
    "#A partir de la matriz de confusion calculamos accuracy, sensitivity y specificity\n",
    "\n",
    "accuracyr=(cmr[0,0]+cmr[1,1])/total1\n",
    "print ('Accuracy : ', accuracyr)\n",
    "\n",
    "sensitivityr = cmr[0,0]/(cmr[0,0]+cmr[0,1])\n",
    "print('Sensitivity : ', sensitivityr )\n",
    "\n",
    "specificityr = cmr[1,1]/(cmr[1,0]+cmr[1,1])\n",
    "print('Specificity : ', specificityr)\n",
    "\n",
    "print (classification_report(y_test, y_predr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X.x1, X.x2, c=y, cmap=plt.cm.coolwarm)\n",
    "plot_svc_decision_function(clfr);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generamos un segundo clasificador svm RADIAL\n",
    "clfr = svm.SVC(kernel='rbf', C=1, gamma=0.05)\n",
    "#Entrenamos el modelo usando los datos de entrenamiento\n",
    "clfr.fit(X_train, y_train)\n",
    "\n",
    "#Pronosticamos la respuesta para el dataset de prueba\n",
    "#En entrenamiento\n",
    "y_predr_e = clfr.predict(X_train)\n",
    "#En prueba\n",
    "y_predr = clfr.predict(X_test)\n",
    "\n",
    "#Accuracy del modelo:  Qué tan frecuentemente el clasificador es correcto?\n",
    "#En entrenamiento: \n",
    "print(\"Accuracy - entrenamiento:\",metrics.accuracy_score(y_train, y_predr_e))\n",
    "#En prueba: \n",
    "print(\"Accuracy - prueba:\",metrics.accuracy_score(y_test, y_predr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Analisis de desempeño del modelo con datos de prueba\n",
    "\n",
    "cmr = confusion_matrix(y_test, y_predr)\n",
    "print('Confusion Matrix : \\n', cmr)\n",
    "total1=sum(sum(cmr))\n",
    "\n",
    "#A partir de la matriz de confusion calculamos accuracy, sensitivity y specificity\n",
    "\n",
    "accuracyr=(cmr[0,0]+cmr[1,1])/total1\n",
    "print ('Accuracy : ', accuracyr)\n",
    "\n",
    "sensitivityr = cmr[0,0]/(cmr[0,0]+cmr[0,1])\n",
    "print('Sensitivity : ', sensitivityr )\n",
    "\n",
    "specificityr = cmr[1,1]/(cmr[1,0]+cmr[1,1])\n",
    "print('Specificity : ', specificityr)\n",
    "\n",
    "print (classification_report(y_test, y_predr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X.x1, X.x2, c=y, cmap=plt.cm.coolwarm)\n",
    "plot_svc_decision_function(clfr);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generamos un TERCER clasificador svm RADIAL\n",
    "clfr = svm.SVC(kernel='rbf', C=1, gamma=1)\n",
    "#Entrenamos el modelo usando los datos de entrenamiento\n",
    "clfr.fit(X_train, y_train)\n",
    "\n",
    "#Pronosticamos la respuesta para el dataset de prueba\n",
    "#En entrenamiento\n",
    "y_predr_e = clfr.predict(X_train)\n",
    "#En prueba\n",
    "y_predr = clfr.predict(X_test)\n",
    "\n",
    "#Accuracy del modelo:  Qué tan frecuentemente el clasificador es correcto?\n",
    "#En entrenamiento: \n",
    "print(\"Accuracy - entrenamiento:\",metrics.accuracy_score(y_train, y_predr_e))\n",
    "#En prueba: \n",
    "print(\"Accuracy - prueba:\",metrics.accuracy_score(y_test, y_predr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X.x1, X.x2, c=y, cmap=plt.cm.coolwarm)\n",
    "plot_svc_decision_function(clfr);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generamos un CUARTO clasificador svm RADIAL\n",
    "clfr = svm.SVC(kernel='rbf', C=1, gamma=5)\n",
    "#Entrenamos el modelo usando los datos de entrenamiento\n",
    "clfr.fit(X_train, y_train)\n",
    "\n",
    "#Pronosticamos la respuesta para el dataset de prueba\n",
    "#En entrenamiento\n",
    "y_predr_e = clfr.predict(X_train)\n",
    "#En prueba\n",
    "y_predr = clfr.predict(X_test)\n",
    "\n",
    "#Accuracy del modelo:  Qué tan frecuentemente el clasificador es correcto?\n",
    "#En entrenamiento: \n",
    "print(\"Accuracy - entrenamiento:\",metrics.accuracy_score(y_train, y_predr_e))\n",
    "#En prueba: \n",
    "print(\"Accuracy - prueba:\",metrics.accuracy_score(y_test, y_predr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X.x1, X.x2, c=y, cmap=plt.cm.coolwarm)\n",
    "plot_svc_decision_function(clfr);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generamos un QUINTO clasificador svm RADIAL\n",
    "clfr = svm.SVC(kernel='rbf', C=10)\n",
    "#Entrenamos el modelo usando los datos de entrenamiento\n",
    "clfr.fit(X_train, y_train)\n",
    "\n",
    "#Pronosticamos la respuesta para el dataset de prueba\n",
    "#En entrenamiento\n",
    "y_predr_e = clfr.predict(X_train)\n",
    "#En prueba\n",
    "y_predr = clfr.predict(X_test)\n",
    "\n",
    "#Accuracy del modelo:  Qué tan frecuentemente el clasificador es correcto?\n",
    "#En entrenamiento: \n",
    "print(\"Accuracy - entrenamiento:\",metrics.accuracy_score(y_train, y_predr_e))\n",
    "#En prueba: \n",
    "print(\"Accuracy - prueba:\",metrics.accuracy_score(y_test, y_predr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X.x1, X.x2, c=y, cmap=plt.cm.coolwarm)\n",
    "plot_svc_decision_function(clfr);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generamos un SEXTO clasificador svm RADIAL\n",
    "clfr = svm.SVC(kernel='rbf', C=10, gamma=0.2)\n",
    "#Entrenamos el modelo usando los datos de entrenamiento\n",
    "clfr.fit(X_train, y_train)\n",
    "\n",
    "#Pronosticamos la respuesta para el dataset de prueba\n",
    "#En entrenamiento\n",
    "y_predr_e = clfr.predict(X_train)\n",
    "#En prueba\n",
    "y_predr = clfr.predict(X_test)\n",
    "\n",
    "#Accuracy del modelo:  Qué tan frecuentemente el clasificador es correcto?\n",
    "#En entrenamiento: \n",
    "print(\"Accuracy - entrenamiento:\",metrics.accuracy_score(y_train, y_predr_e))\n",
    "#En prueba: \n",
    "print(\"Accuracy - prueba:\",metrics.accuracy_score(y_test, y_predr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X.x1, X.x2, c=y, cmap=plt.cm.coolwarm)\n",
    "plot_svc_decision_function(clfr);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hacemos tuning de los parametros del SVM RADIAL: Costo y gamma\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Set the parameters by cross-validation\n",
    "tuned_parameters = [{'kernel': ['rbf'], 'gamma': [0.05, 0.1, 0.2, 0.5], 'C': [0.1, 1, 5, 10]}]\n",
    "scores = ['precision', 'recall']\n",
    "\n",
    "for score in scores:\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "\n",
    "    clf = GridSearchCV(\n",
    "        SVC(), tuned_parameters, cv=5, scoring='%s_macro' % score\n",
    "    )\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    y_true, y_pred = y_test, clf.predict(X_test)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clasificador svm RADIAL FINAL\n",
    "clfr = svm.SVC(kernel='rbf', C=5, gamma=0.05)\n",
    "#Entrenamos el modelo usando los datos de entrenamiento\n",
    "clfr.fit(X_train, y_train)\n",
    "\n",
    "#Pronosticamos la respuesta para el dataset de prueba\n",
    "#En entrenamiento\n",
    "y_predr_e = clfr.predict(X_train)\n",
    "#En prueba\n",
    "y_predr = clfr.predict(X_test)\n",
    "\n",
    "#Accuracy del modelo:  Qué tan frecuentemente el clasificador es correcto?\n",
    "#En entrenamiento: \n",
    "print(\"Accuracy - entrenamiento:\",metrics.accuracy_score(y_train, y_predr_e))\n",
    "#En prueba: \n",
    "print(\"Accuracy - prueba:\",metrics.accuracy_score(y_test, y_predr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Analisis de desempeño del modelo con datos de prueba\n",
    "\n",
    "cmr = confusion_matrix(y_test, y_predr)\n",
    "print('Confusion Matrix : \\n', cmr)\n",
    "total1=sum(sum(cmr))\n",
    "\n",
    "#A partir de la matriz de confusion calculamos accuracy, sensitivity y specificity\n",
    "\n",
    "accuracyr=(cmr[0,0]+cmr[1,1])/total1\n",
    "print ('Accuracy : ', accuracyr)\n",
    "\n",
    "sensitivityr = cmr[0,0]/(cmr[0,0]+cmr[0,1])\n",
    "print('Sensitivity : ', sensitivityr )\n",
    "\n",
    "specificityr = cmr[1,1]/(cmr[1,0]+cmr[1,1])\n",
    "print('Specificity : ', specificityr)\n",
    "\n",
    "print (classification_report(y_test, y_predr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X.x1, X.x2, c=y, cmap=plt.cm.coolwarm)\n",
    "plot_svc_decision_function(clfr);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hacemos tuning de los parametros del SVM POLINOMIAL: Costo, gamma, degree\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Set the parameters by cross-validation\n",
    "tuned_parameters = [{'kernel': ['poly'], 'gamma': [0.05, 0.1, 0.2, 0.5], 'C': [0.1, 1, 5, 10], 'degree' : [2, 3] }]\n",
    "scores = ['precision', 'recall']\n",
    "\n",
    "for score in scores:\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "\n",
    "    clf = GridSearchCV(\n",
    "        SVC(), tuned_parameters, scoring='%s_macro' % score\n",
    "    )\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    y_true, y_pred = y_test, clf.predict(X_test)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clasificador svm POLINOMIAL FINAL\n",
    "clfr = svm.SVC(kernel='poly', C=5, gamma=0.05, degree=2)\n",
    "#Entrenamos el modelo usando los datos de entrenamiento\n",
    "clfr.fit(X_train, y_train)\n",
    "\n",
    "#Pronosticamos la respuesta para el dataset de prueba\n",
    "#En entrenamiento\n",
    "y_predr_e = clfr.predict(X_train)\n",
    "#En prueba\n",
    "y_predr = clfr.predict(X_test)\n",
    "\n",
    "#Accuracy del modelo:  Qué tan frecuentemente el clasificador es correcto?\n",
    "#En entrenamiento: \n",
    "print(\"Accuracy - entrenamiento:\",metrics.accuracy_score(y_train, y_predr_e))\n",
    "#En prueba: \n",
    "print(\"Accuracy - prueba:\",metrics.accuracy_score(y_test, y_predr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Analisis de desempeño del modelo con datos de prueba\n",
    "\n",
    "cmr = confusion_matrix(y_test, y_predr)\n",
    "print('Confusion Matrix : \\n', cmr)\n",
    "total1=sum(sum(cmr))\n",
    "\n",
    "#A partir de la matriz de confusion calculamos accuracy, sensitivity y specificity\n",
    "\n",
    "accuracyr=(cmr[0,0]+cmr[1,1])/total1\n",
    "print ('Accuracy : ', accuracyr)\n",
    "\n",
    "sensitivityr = cmr[0,0]/(cmr[0,0]+cmr[0,1])\n",
    "print('Sensitivity : ', sensitivityr )\n",
    "\n",
    "specificityr = cmr[1,1]/(cmr[1,0]+cmr[1,1])\n",
    "print('Specificity : ', specificityr)\n",
    "\n",
    "print (classification_report(y_test, y_predr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X.x1, X.x2, c=y, cmap=plt.cm.coolwarm)\n",
    "plot_svc_decision_function(clfr);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "clf = svm.SVC(kernel='rbf', random_state=101)\n",
    "\n",
    "C_options = np.array([0.5, 0.8, 1, 1.2, 1.5])\n",
    "gamma_options = np.array([0.8, 0.9, 1.0, 1.1, 1.2])\n",
    "\n",
    "hyperparameter_search_space = [{\n",
    "    'kernel': ['rbf'], \n",
    "    'C': C_options, \n",
    "    'gamma': gamma_options\n",
    "}]\n",
    "\n",
    "gridsearch = GridSearchCV(estimator=clf, \n",
    "                          param_grid=hyperparameter_search_space, \n",
    "                          cv=5)\n",
    "gridsearch.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "cv_performance = gridsearch.best_score_\n",
    "test_performance = gridsearch.score(X_test, y_test)\n",
    "\n",
    "\n",
    "print('Best parameter: {}'.format(str(gridsearch.best_params_)))\n",
    "print('Cross-validation accuracy score: {0:0.3f}'.format(cv_performance))\n",
    "print('Test accuracy score: {0:0.3f}'.format(test_performance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clasificador svm RADIAL FINAL 2\n",
    "clfr = svm.SVC(kernel='rbf', C=1, gamma=0.8)\n",
    "#Entrenamos el modelo usando los datos de entrenamiento\n",
    "clfr.fit(X_train, y_train)\n",
    "\n",
    "#Pronosticamos la respuesta para el dataset de prueba\n",
    "#En entrenamiento\n",
    "y_predr_e = clfr.predict(X_train)\n",
    "#En prueba\n",
    "y_predr = clfr.predict(X_test)\n",
    "\n",
    "#Accuracy del modelo:  Qué tan frecuentemente el clasificador es correcto?\n",
    "#En entrenamiento: \n",
    "print(\"Accuracy - entrenamiento:\",metrics.accuracy_score(y_train, y_predr_e))\n",
    "#En prueba: \n",
    "print(\"Accuracy - prueba:\",metrics.accuracy_score(y_test, y_predr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
